{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting to explore daily train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOOP 0, DATE: 2021-10-27\n",
      "TRAIN NUMBER 15, 0.028666666666666668\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27592/2036476641.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 1, DATE: 2021-09-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 2, DATE: 2021-08-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 3, DATE: 2021-07-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 4, DATE: 2021-06-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 5, DATE: 2021-05-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 6, DATE: 2021-04-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 7, DATE: 2021-03-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 8, DATE: 2021-02-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 9, DATE: 2021-01-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 10, DATE: 2020-12-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "LOOP 11, DATE: 2020-11-27\n",
      "TRAIN NUMBER 749, 0.99866666666666675\n",
      "DONE MAKING DATAFRAME\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Hack for using import with relative path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fetchTrainData import fetch_train_data\n",
    "from fetchWeatherData import fetch_weather_data, save_weather_data, open_weather_data\n",
    "from twitterBot import tweet_station_congested\n",
    "\n",
    "weather_data = {}\n",
    "\n",
    "def get_weather(stop_):\n",
    "    tag = stop_['station.shortCode']\n",
    "    if type(tag) == str:\n",
    "        lat = stop_['station.location'][0]\n",
    "        lon = stop_['station.location'][1]\n",
    "        if tag not in weather_data:\n",
    "            print(\"FETCHING DATA FOR: \" + tag, end = \"\\r\")\n",
    "            weather_data[tag] = fetch_weather_data(unix_time, lat, lon)['list'][0]['main']['temp']\n",
    "\n",
    "            \n",
    "def extract_time_tables(train_stops_):\n",
    "    print(\"TRAIN NUMBER \" + str(train_stops_.name) + \", \" + str(float(train_stops_.name) / number_of_trains), end = \"\\r\")\n",
    "    train_stops = pd.json_normalize(train_stops_)\n",
    "    to_return = train_stops['differenceInMinutes'].sum()\n",
    "    train_stops = train_stops.drop(['differenceInMinutes'], axis=1)\n",
    "\n",
    "    train_stops.apply(get_weather, axis=1)\n",
    "\n",
    "    for stop in train_stops['station.shortCode'].unique():\n",
    "        if type(stop) == str:\n",
    "            if stop not in df.columns:\n",
    "                df[stop] = np.nan\n",
    "\n",
    "            df[stop][train_stops_.name] = weather_data[stop]\n",
    "        \n",
    "    \n",
    "    return to_return\n",
    "\n",
    "loops = 12 # How many months data we want in the dataframe, apparently we can't get over 12 months data\n",
    "number_of_trains = 750 # How many trains to fetch per month. Seems to be softlocked at about 1000, 500 is safe\n",
    "all_data_frames = []\n",
    "all_late_amounts = []\n",
    "\n",
    "\n",
    "for i in range(loops):\n",
    "    date = datetime.date.today() - relativedelta(months = i)\n",
    "    unix_time = time.mktime(date.timetuple())\n",
    "    print()\n",
    "    print(\"LOOP \" + str(i) + \", DATE: \" + str(date))\n",
    "\n",
    "    q = (\n",
    "        \"\"\" \n",
    "        {\n",
    "        \"\"\"\n",
    "          f'  trainsByDepartureDate(departureDate: \\\"{date}\\\",'\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "          f'  take: {number_of_trains},'\n",
    "        \"\"\"\n",
    "            where: {\n",
    "                and: [\n",
    "                    {or: [{deleted: {unequals: null}}, \n",
    "                    {deleted: {equals: false}}]}, \n",
    "                    {cancelled: {equals: false}}, \n",
    "                    {operator: {shortCode: {equals: \"vr\"}}}\n",
    "                    ]  \n",
    "            }\n",
    "          ) \n",
    "          {\n",
    "            trainNumber\n",
    "            departureDate\n",
    "            timeTableRows {\n",
    "            differenceInMinutes\n",
    "              station {\n",
    "                shortCode\n",
    "                location\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    result = fetch_train_data(q)\n",
    "    recs = result['data']['trainsByDepartureDate']\n",
    "\n",
    "    df = pd.json_normalize(recs)\n",
    "    all_stops = pd.json_normalize(df['timeTableRows'])\n",
    "    df = df.drop(['timeTableRows'], axis=1)\n",
    "\n",
    "    weather_data = open_weather_data(date)\n",
    "    how_much_late = all_stops.apply(extract_time_tables, axis=1)\n",
    "    save_weather_data(date, weather_data)\n",
    "    \n",
    "    df['departureDate'] = date.month\n",
    "    \n",
    "    all_data_frames.append(df)\n",
    "    all_late_amounts.append(how_much_late)\n",
    "    \n",
    "combined_train_data = pd.concat(all_data_frames).fillna(0).reset_index().drop(['index'], axis=1)\n",
    "combined_late_amounts = pd.concat(all_late_amounts).fillna(0).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "combined_late_amounts.rename(columns={0: \"total_late\"}, inplace=True)\n",
    "combined_late_amounts = combined_late_amounts[\"total_late\"].apply(lambda x: int(x > 4))\n",
    "\n",
    "print()\n",
    "print(\"DONE MAKING DATAFRAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST SCORE: 0.7788888888888889\n",
      "NEW BEST SCORE: 0.785\n",
      "NEW BEST SCORE: 0.7894444444444444\n",
      "0.7755555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, model_selection, pipeline, preprocessing\n",
    "\n",
    "best_score = 0\n",
    "best_reg = linear_model.SGDClassifier(max_iter=1000)\n",
    "\n",
    "for i in range(100):\n",
    "    print(i, end = \"\\r\")\n",
    "    training_data, test_data, train_target, test_target = model_selection.train_test_split(combined_train_data, combined_late_amounts, train_size=0.8)\n",
    "    train_target = np.ravel(train_target)\n",
    "    test_target = np.ravel(test_target)\n",
    "\n",
    "    reg = linear_model.SGDClassifier(max_iter=1000)\n",
    "    reg.fit(training_data, train_target)\n",
    "    reg_score = reg.score(test_data, test_target)\n",
    "    if reg_score > best_score:\n",
    "        best_score = reg_score\n",
    "        best_reg = reg\n",
    "        print(\"NEW BEST SCORE: \" + str(best_score))\n",
    "        \n",
    "print(best_reg.score(test_data, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_27592/2458956676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[stop] = np.nan\n",
      "/tmp/ipykernel_27592/2458956676.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[station] = 0.0\n",
      "/home/otters/git/VRbad.io/venv/lib/python3.8/site-packages/sklearn/base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEETED ABOUT Helsinki BEING CONGESTED\n",
      "TWEETED ABOUT Kerava BEING CONGESTED\n",
      "TWEETED ABOUT Joensuu BEING CONGESTED\n",
      "TWEETED ABOUT Hämeenlinna BEING CONGESTED\n",
      "TWEETED ABOUT Oulu BEING CONGESTED\n",
      "TWEETED ABOUT Seinäjoki BEING CONGESTED\n"
     ]
    }
   ],
   "source": [
    "def extract_time_tables_station(train_stops_):\n",
    "    train_stops = pd.json_normalize(train_stops_)\n",
    "\n",
    "    train_stops.apply(get_weather, axis=1)\n",
    "\n",
    "    for stop in train_stops['station.shortCode'].unique():\n",
    "        if type(stop) == str:\n",
    "            if stop not in df.columns:\n",
    "                df[stop] = np.nan\n",
    "\n",
    "            df[stop][train_stops_.name] = weather_data[stop]\n",
    "            \n",
    "# List here stations that are checked, and possible tweeted about\n",
    "stations_to_check = [('HKI', 'Helsinki'), ('PSL', 'Pasila'), ('TKL', 'Tikkurila'), ('KE', 'Kerava'), \n",
    "                     ('JNS', 'Joensuu'), ('TPE', 'Tampere'), ('TKU', 'Turku'), ('HL', 'Hämeenlinna'), \n",
    "                     ('OL', 'Oulu'), ('SK', 'Seinäjoki'), ('JY', 'Jyväskylä'), ('KV', 'Kouvola')]\n",
    "results = []\n",
    "\n",
    "\n",
    "for station in stations_to_check:\n",
    "    date = datetime.date.today()\n",
    "    unix_time = time.mktime(date.timetuple())\n",
    "    \n",
    "    q = (\n",
    "        \"\"\" \n",
    "        {\n",
    "        \"\"\"\n",
    "          f'  trainsByStationAndQuantity(station: \"{station[0]}\",'\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "          f'  take: {200},'\n",
    "        \"\"\"\n",
    "            where: {\n",
    "                and: [\n",
    "                    {or: [{deleted: {unequals: null}}, \n",
    "                    {deleted: {equals: false}}]}, \n",
    "                    {cancelled: {equals: false}}, \n",
    "                    {operator: {shortCode: {equals: \"vr\"}}}\n",
    "                    ]  \n",
    "            }\n",
    "          ) \n",
    "          {\n",
    "            trainNumber\n",
    "            departureDate\n",
    "            timeTableRows {\n",
    "              station {\n",
    "                shortCode\n",
    "                location\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "        )\n",
    "    \n",
    "    result = fetch_train_data(q)\n",
    "    recs = result['data']['trainsByStationAndQuantity']\n",
    "\n",
    "    df = pd.json_normalize(recs)\n",
    "    all_stops = pd.json_normalize(df['timeTableRows'])\n",
    "    df = df.drop(['timeTableRows'], axis=1)\n",
    "    \n",
    "    weather_data = open_weather_data(date)\n",
    "    all_stops.apply(extract_time_tables_station, axis=1)\n",
    "    save_weather_data(date, weather_data)\n",
    "    \n",
    "    df['departureDate'] = date.month\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    for station in best_reg.feature_names_in_:\n",
    "        if station not in df.columns:\n",
    "            df[station] = 0.0\n",
    "            \n",
    "    for station in df.columns:\n",
    "        if station not in best_reg.feature_names_in_:\n",
    "            df.drop([station], inplace=True, axis=1)\n",
    "    \n",
    "    how_many_late = 0\n",
    "    predictions = best_reg.predict(df)\n",
    "    for prediction in predictions:\n",
    "        if prediction == 1:\n",
    "            how_many_late = how_many_late + 1\n",
    "            \n",
    "    results.append(round((how_many_late / len(predictions)) * best_score, 2))  # What percent of trains does the algorithm think are going to be late\n",
    "\n",
    "tweet = True # Should results be tweeted about, or just printed\n",
    "    \n",
    "if tweet:\n",
    "    for i, val in enumerate(stations_to_check):\n",
    "        if results[i] > 0.65:\n",
    "            tweet_station_congested(val[1], results[i])\n",
    "            print(f\"TWEETED ABOUT {val[1]} BEING CONGESTED\")\n",
    "else:\n",
    "    print(stations_to_check)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5d966f8ef4220096744806d83432b9a228778896d08cdcbc2e266632b99e2da"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
